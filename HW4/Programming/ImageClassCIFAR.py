# -*- coding: utf-8 -*-
"""A4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kigW7sINo26XBPNu8NpAbgGI3jacr26c

Here we will use make use of the same helper functions from problem A3 to evaluate
"""

import math 
import matplotlib.pyplot as plt              # For plotting
import seaborn as sns                        # For styling plots
import torch                                 # Overall PyTorch import
import torch.nn as nn                        # For specifying neural networks
import torch.nn.functional as F              # For common functions
import torch.optim as optim                  # For optimizizing model parameter
import torchvision as tv
import numpy as np 
from torchvision import datasets, models, transforms

"""Hyperparamters to Tune"""

INPUT_SIZE    = 3 * 32 * 32   # An image has 32 x 32 pixels each with Red/Green/Blue values. 
NUM_CLASSES   = 10            # The number of output classes. In this case, from 0 to 9
NUM_EPOCHS    = 12            # The number of times we loop over the whole dataset during training
BATCH_SIZE    = 100          # The size of input data took for one iteration of an epoch
LEARNING_RATE = 1e-3          # The speed of convergence

"""Upload CIFAR 10 Data and Split Training set in training + validation sets

"""

transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = datasets.CIFAR10(root='./data', train=True, download=True, 
                            transform=transform)
# Split into 80:20 Training/Validation sets
trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])

train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, 
                                           shuffle=True, num_workers=2)

val_loader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, 
                                           shuffle=False, num_workers=2)

testset = datasets.CIFAR10(root='./data', train=False, download=True, 
                           transform=transform)

test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, 
                                          shuffle=False, num_workers=2)

"""Helper Functions to train models, calculate accuracy and loss functions"""

def train(model, train_loader, val_loader,
          num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
          compute_accs = False, Batches = False): 
    """
    This function uses the model to train on the training data for the 
    specified number of epochs. If compute_accs is set to True, the training 
    and validation accuracies 

    Args:
        model: Modified Alexnet Model
        train_loader, val_loader: The pytorch dataset loaders for the trainst and valset
        num_epochs: The number of times to loop over the batches in train_loader
        learning_rate: The learning rate for the optimizer
        compute_accs: A bool flag for whether or not this function should compute the train and validation
                      accuracies at the end of each epoch. This feature is useful for visualizing
                      how the model is learning, but slows down training time.
    Returns:
        The train and validation accuracies if compute_accs is True, None otherwise
    """

    # Create loss Function

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    train_accs = [] 
    val_accs = [] 

    for epoch in range(num_epochs): 
        for i, (img, labels) in enumerate(train_loader): 

            # If you are using a GPU, speed up computation by moving values to the GPU
            if torch.cuda.is_available():
                model = model.cuda()
                img = img.cuda()
                labels = labels.cuda()

            optimizer.zero_grad()               # Reset gradient for next computation
            outputs = model(img)               # Forward pass: compute the output class given a image
            loss = criterion(outputs, labels)   # Compute loss: difference between the pred and true
            loss.backward()                     # Backward pass: compute the weight
            optimizer.step()                    # Optimizer: update the weights of hidden nodes

            if Batches: 
                if (i + 1) % 100 == 0:  # Print every 100 batches
                    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], '
                          f'Loss: {loss.item():.4f}')
                

        train_acc = accuracy(model,train_loader)
        val_acc = accuracy(model,val_loader)

        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Acc {100 * train_acc:.2f}%, Validation Acc {100 * val_acc:.2f}%')
        
        train_accs.append(train_acc)
        val_accs.append(val_acc)

    if compute_accs:
        return train_accs, val_accs
    else:
        return None

def accuracy(model, data_loader):
    """
    For a given data_loader, evaluate the model on the dataset and compute its classification
    accuracy.

    Args:
        model: The neural network to train (Alexnet)
        data_loader: A dataset loader for some dataset.
    Returns:
        The classificiation accuracy of the model on this dataset.
    """
    correct = 0
    total = 0
    for images, labels in data_loader:
        if  torch.cuda.is_available():
            images = images.cuda()
            labels = labels.cuda()

        outputs = model(images)                           # Make predictions
        _, predicted = torch.max(outputs.data, 1)       # Choose class with highest scores
        total += labels.size(0)                         # Increment the total count
        correct += (predicted == labels).sum().item()   # Increment the correct count

    return correct / total

def plot(train_acc, val_acc, title):
    """
    Plots and training and validation accuracies over times

    Args: 
        train_acc, val_acc: list of training/validation accuracies over training time

    """

    plt.figure(figsize = (16,10))
    epochs = list(range(1, len(train_acc) + 1))

    val = plt.plot(epochs, val_acc,
                     '--', label= 'Validation')
    
    plt.plot(epochs, train_acc, color=val[0].get_color(),
               label= 'Train')
    
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(title)
    plt.legend()
    plt.xlim([1,max(epochs)])

def plot_hyperparam(num_row, num_cols, histories, titles): 
    """
    Plots the accuracy vs epochs of each hyperparameter setting

    Args: 
      num_row: number of rows for subplots 
      num_cols: number of cols for subplots 
      histories: training and Validation accuracies 
      titles: Titles used for each subplot

    """

    fig, axes = plt.subplots(num_row, num_cols, figsize=(16, 8), 
                         sharex = True, sharey = True)
    
    for acc, title, ax in zip(histories, titles, axes.ravel()): 
        # Plot Accuracy vs Epochs
        epochs = list(range(1, len(acc[0]) + 1))

        train_history = acc[0]
        val_history = acc[1]

        ax.plot(epochs, train_history, label= 'Train')
        ax.plot(epochs, val_history, label= 'Validation')
        
        
        ax.set_title(title)

    ax.legend()
        
    # set labels
    plt.setp(axes[-1, :], xlabel='Epochs')
    plt.setp(axes[:, 0], ylabel='Accuracy')
    plt.tight_layout()
    plt.show()

    return fig, axes

"""The following code blocks will define differnent network architectures as defined in the homework specification."""

# Net A
# Fully connected output with no hidden layers. Logistic Regression 
class NetA(nn.Module):
    def __init__(self):
        super(NetA, self).__init__()
        self.fc = nn.Linear(INPUT_SIZE, NUM_CLASSES)
    def forward(self, x):
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


# Net B
# Full connected output with one hidden layer (M is ihyperparameter)
class NetB(nn.Module):
    def __init__(self, M):
        super(NetB, self).__init__()
        self.forward1 = nn.Sequential(
            nn.Linear(INPUT_SIZE, M),
            nn.ReLU(),
            nn.Linear(M, NUM_CLASSES)
        )

    def forward(self, x): 
        x = torch.flatten(x, 1)
        return self.forward1(x)

# Net C
# Convolutional layer with max-pool and fully-connected output
class NetC(nn.Module):
    '''
    Args: 
        M : 'Int' 
            Size of hidden layer
        N : 'Int' 
            Size of filter
        k : 'Int' 
            Size of maxpool      
    '''
    def __init__(self, M, N, k):
        super(NetC, self).__init__()
        finLinSize= int( ((33 - k) / N)**2 * M) # Final Linear Size 
        self.conv = nn.Conv2d(3,M,k)
        self.pool = nn.MaxPool2d(N) 
        self.fc = nn.Linear(finLinSize, 10)

    def forward(self, x): 
      x = self.pool(F.relu(self.conv(x)))
      x = torch.flatten(x, 1)
      x = self.fc(x)
      return x

# Net D
# Modified 

class NetD(nn.Module):
    '''
    Tutorial Neural net classifier 
    Args
        n1: Number of channels for first convolution network
        k2: Kernel size of first convolution layer 
        str_1: Stride for first convolution layer 
        p1: padding for first convolution layer

        k_pool1: kernel size for first max pool 
        str_pool1: stride s for first max pool 
        p_p1: padding for first max pool 

        n2: Number of channels for second convolution layer
        k2: kernel size of second convolution layer  
        str_2: Stride for second convolution layer 
        p2: padding for second convolution layer

        k_pool2: kernel size for second max pool 
        str_pool2: stride s for second max pool 
        p_p2: padding for second max pool

        l1: Fully connected layer 1 dimension 
        l2 : Fully connected layer 2 dimension

    '''
    def __init__(self, n1 = 32, k1 = 5, str_1 = 1, p1 = 0,  
                 k_pool1 = 2, str_pool1 = 2, p_p1 = 0, 
                 n2 = 32, k2 =3, str_2 = 1, p2 = 0, 
                 k_pool2 = 2, str_pool2 = 2, p_p2 = 0, 
                 l1 = 288, l2 = 144):
      
        super(NetD, self).__init__()
        # Hyperparameters Calculations
        conv_dim1 = (n1 - k1 + 2*p1)/str_1 + 1
        conv_pool1 = (conv_dim1-k_pool1 + 2*p_p1)/str_pool1 + 1

        conv_dim2 = (conv_pool1-k2 + 2*p2)/str_2 + 1 
        conv_pool2 = int((conv_dim2-k_pool2 + 2*p_p2)/str_pool2 + 1)

        self.conv1 = nn.Conv2d(3, n1, k1, stride = str_1, padding = p1)
        self.conv2 = nn.Conv2d(n1,n2,k2,stride = str_2, padding = p2)
        self.fc1 = nn.Linear(n2*conv_pool2*conv_pool2, l1)
        self.fc2 = nn.Linear(l1,l2)
        self.fc3 = nn.Linear(l2,NUM_CLASSES)

        self.kpool1 = k_pool1
        self.kpool2 = k_pool2
        self.strp1 = str_pool1
        self.strp2 = str_pool2

        self.p_p1 = p_p1
        self.p_p2 = p_p2 

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = F.max_pool2d(x,self.kpool1,stride = self.strp1, padding = self.p_p1)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x,self.kpool2,stride = self.strp2, padding = self.p_p2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        return x

"""A4 Part a) 
Note since there are no hyperparameters, we only perform a simple 
"""

neta = NetA()

train_history, val_history = train(neta, train_loader, val_loader, 
                                        num_epochs=NUM_EPOCHS, 
                                        learning_rate=LEARNING_RATE, 
                                        compute_accs=True)

plot(train_history, val_history, title = 'Linear Network')

print('Test Accuracy with Batches is: %.5f' % accuracy(neta, test_loader))

"""A4 part b) we vary the hyperparameter M and plot each accuracy for the M parameter. """

M_lis = list(range(100, 5000, 500)) # Increment M by 100

titles = [] 
histories = []

for M in M_lis: 
    netb = NetB(M)
    print(f'M = {M}')
    print('-'*15)
    print('\n')
    train_history, val_history = train(netb, train_loader, val_loader, 
                                        num_epochs=NUM_EPOCHS, 
                                        learning_rate=LEARNING_RATE, 
                                        compute_accs=True)
    
    history = [train_history, val_history]
    title = f'M = {M}'

    histories.append(history)
    titles.append(title)

    print('-'*15)
    print('Test Accuracy with M = %d is: %.5f' % (M, accuracy(netb, test_loader)))
    print('-'*15)
    print('\n\n\n')

plot_hyperparam(2, 5, histories, titles)

"""A5 part c) """

M_lis = list(range(100, 2100, 500)) # Increment M by 100

filters = [3, 5, 7] # size of filters
N1 = [2, 3, 5] # Max pool size for first k 
N2 = [2, 4, 7] # Max pool size for second k 
N3 = [2, 13] # Max pool size for third k 

Pool = [N1, N2, N3]

titles = [] 
histories = []

for M in M_lis: 
    i = 0
    for k in filters: 
        for N in Pool[i]: 
            netc = NetC(M, N, k)
            print( f'M = {M}, k = {k}, N = {N}')
            print('-'*15)
            print('\n')
            train_history, val_history = train(netc, train_loader, val_loader, 
                                        num_epochs=8, 
                                        learning_rate=LEARNING_RATE, 
                                        compute_accs=True)
            
            history = [train_history, val_history]
            title = f'M = {M}, k = {k}, N = {N}'

            histories.append(history)
            titles.append(title)

            print('\n\n')

        i += 1

plot_hyperparam(4, 8, histories, titles)

# Run NetC with the best hyperparameters 

netc = NetC(M = 1100, N = 5, k = 3)
M = 1100
k = 3
N = 5
print( f'M = {M}, k = {k}, N = {N}')
print('-'*15)
print('\n')
train_history, val_history = train(netc, train_loader, val_loader, 
                            num_epochs=20, 
                            learning_rate=LEARNING_RATE, 
                            compute_accs=True)

print('-'*15)
print('Test Accuracy with NetC is: %.5f' %  accuracy(netc, test_loader))
print('-'*15)
print('\n\n\n')

"""Part d) """

# Fine Tuning A4d
      
n1s = [32]
n2s = [32, 64, 128]
l1s = [288, 144]
l2 = 144

titles = [] 
histories = []

for n1 in n1s: 
    for n2 in n2s:
        for l1 in l1s: 
            netd = NetD(n1 = n1, k1 = 5, str_1 = 1, p1 = 0,  
                    k_pool1 = 2, str_pool1 = 2, p_p1 = 0, 
                    n2 = n2, k2 =3, str_2 = 1, p2 = 0, 
                    k_pool2 = 2, str_pool2 = 2, p_p2 = 0, 
                    l1 = l1, l2 = l2)
            
            print(f'n1 = {n1},n2 = {n2},l1 = {l1},l2 = {l2}')
            print('-'*15)
            print('\n')
            train_history, val_history = train(netd, train_loader, val_loader, 
                                        num_epochs=8, 
                                        learning_rate=LEARNING_RATE, 
                                        compute_accs=True)
            history = [train_history, val_history]
            title = f'n1 = {n1},n2 = {n2},l1 = {l1},l2 = {l2}'

            histories.append(history)
            titles.append(title)

            print('\n\n')

plot_hyperparam(2, 3, histories, titles)

NUM_EPOCHS = 20 

netd = NetD()

train_history, val_history = train(netd, train_loader, val_loader, 
                                        num_epochs=NUM_EPOCHS, 
                                        learning_rate=LEARNING_RATE, 
                                        compute_accs=True)

plot(train_history, val_history, title = 'Conv Filter = 32, KernelSize1 = 5, Kernel Size2 = 3')
print('-'*15)
print('Test Accuracy with NetD is: %.5f' %  accuracy(netd, test_loader))
print('-'*15)
print('\n\n\n')

"""Part E) Adverserial Attack"""

def imshow(img, perturbed, truelab, perlab):
  img = img / 2 + 0.5   # unnormalize
  perturbed =  perturbed/2 + 0.5 # unnormalize

  npimg = img.cpu().detach().numpy()   # convert from tensor
  npper = perturbed.cpu().detach().numpy()   # convert from tensor

  fig, (ax1, ax2) = plt.subplots(1, 2)
  ax1.imshow(np.transpose(npimg, (1, 2, 0))) 
  ax1.set_title(truelab)
  ax1.axis('off')

  ax2.imshow(np.transpose(npper, (1, 2, 0))) 
  ax2.set_title(perlab)
  ax2.axis('off')

  plt.show()

# FGSM attack code
def fgsm_attack(image, epsilon, data_grad):
    # Collect the element-wise sign of the data gradient
    sign_data_grad = data_grad/torch.abs(data_grad)
    # Create the perturbed image by adjusting each pixel of the input image
    perturbed_image = image + epsilon*sign_data_grad
    # Adding clipping to maintain [0,1] range
    # perturbed_image = torch.clamp(perturbed_image, 0, 1)
    # Return the perturbed image
    return perturbed_image

# Implement Attack

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog',
    'frog', 'horse', 'ship', 'truck')

train_loader = torch.utils.data.DataLoader(trainset, batch_size=1, 
                                           shuffle=False, num_workers=2)

i = 0
model = netd

for img, labels in train_loader:

    # Send img to GPU
    img = img.cuda()
    labels = labels.cuda()

    # Set requires_grad attribute of tensor. Important for Attack
    img.requires_grad = True

    outputs = model(img)                         # Make predictions
    _, predicted = torch.max(outputs.data, 1)       # Choose class with highest scores

    # If model predicts incorrectly, continue for loop
    if predicted != labels: 
        continue 

    # Calculate the loss
    criterion = nn.CrossEntropyLoss()
    loss = criterion(outputs, labels)

    # Zero all existing gradients
    model.zero_grad()

    # Calculate gradients of model in backward pass
    loss.backward()

    # Collect datagrad
    img_grad = img.grad.data

    # Call FGSM Attack
    perturbed_data = fgsm_attack(img, 0.01, img_grad)

    # Re-classify the perturbed image
    output_p = model(perturbed_data)

    # Prediction of perturbed image
    _, predictedP = torch.max(output_p.data, 1)  

    TrueLab = f"Label = {classes[predicted.item()]}"
    PertLab = f"Perturbed Label = {classes[predictedP.item()]}"   

    imshow(tv.utils.make_grid(img), tv.utils.make_grid(perturbed_data), 
           TrueLab, PertLab) 
    
    i +=1 

    if i > 3: 
        break