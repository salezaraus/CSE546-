# -*- coding: utf-8 -*-
"""TopicModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_TCHhUr87PugO3ynkfHjFiQucp0VCvet
"""

from google.colab import drive
drive.mount('/content/drive')

"""Helper Function """

import codecs
import json
import torch
import torch.nn as nn   
import numpy as np
from itertools import combinations
from operator import is_not
from typing import Any, Dict, List, Tuple
from functools import partial
from scipy import sparse
import os
import matplotlib.pyplot as plt

os.chdir('/content/drive/MyDrive/CSE 546 /HW4/Programming/helper')


def load_sparse(input_filename):
    npy = np.load(input_filename)
    coo_matrix = sparse.coo_matrix((npy['data'], (npy['row'], npy['col'])), shape=npy['shape'])
    return coo_matrix.tocsc()


def read_json(input_filename):
    with codecs.open(input_filename, 'r', encoding='utf-8') as input_file:
        data = json.load(input_file, encoding='utf-8')
    return data


def extract_topics(B : torch.Tensor, vocabulary: List[str], k: int = 20) -> List[Tuple[str, List[str]]]:
        """
        Given the learned (K, vocabulary size) weights, print the
        top k words from each row as a topic.

        Parameters
        ----------
        B: ``torch.Tensor``   T x V
            The weight matrix whose second dimension equals the vocabulary size.
        k: ``int``
            The number of words per topic to display.

        Returns
        -------
        topics: ``List[Tuple[str, List[int]]]``
            collection of learned topics
        """
        topics = []

        for i, topic in enumerate(B):
            word_strengths = list(zip(vocabulary, topic.tolist()))
            sorted_by_strength = sorted(word_strengths,
                                        key=lambda x: x[1],
                                        reverse=True)
            top_k = [x[0] for x in sorted_by_strength][:k]
            topics.append((str(i), top_k))

        return topics


def compute_npmi(npmi_numerator, npmi_denominator, n_docs, ref_vocab_index, topics, num_words=10):
    """
    Compute global NPMI across topics

    Parameters
    ----------
    topics: ``List[Tuple[str, List[int]]]``
        list of learned topics
    num_words: ``int``
        number of words to compute npmi over
    """
    topics_idx = [[ref_vocab_index.get(word)
                   for word in topic[1][:num_words]] for topic in topics]
    rows = []
    cols = []
    res_rows = []
    res_cols = []
    max_seq_len = max([len(topic) for topic in topics_idx])

    for index, topic in enumerate(topics_idx):
        topic = list(filter(partial(is_not, None), topic))
        if len(topic) > 1:
            _rows, _cols = zip(*combinations(topic, 2))
            res_rows.extend([index] * len(_rows))
            res_cols.extend(range(len(_rows)))
            rows.extend(_rows)
            cols.extend(_cols)

    npmi_data = ((np.log10(n_docs) + npmi_numerator[rows, cols])
                 / (np.log10(n_docs) - npmi_denominator[rows, cols]))
    npmi_data[npmi_data == 1.0] = 0.0
    npmi_shape = (len(topics), len(list(combinations(range(max_seq_len), 2))))
    npmi = sparse.csr_matrix((npmi_data.tolist()[0], (res_rows, res_cols)), shape=npmi_shape)
    return npmi.mean()


def generate_npmi_vals(interactions, document_sums):
    """
    Compute npmi values from interaction matrix and document sums
    Parameters
    ----------
    interactions: ``np.ndarray``
        Interaction matrix of size reference vocab size x reference vocab size,
        where cell [i][j] indicates how many times word i and word j co-occur
        in the corpus.
    document_sums: ``np.ndarray``
        Matrix of size number of docs x reference vocab size, where
        cell [i][j] indicates how many times word i occur in documents
        in the corpus
    """
    interaction_rows, interaction_cols = interactions.nonzero()
    # generating doc sums...
    doc_sums = sparse.csr_matrix((np.log10(document_sums[interaction_rows])
                                  + np.log10(document_sums[interaction_cols]),
                                  (interaction_rows, interaction_cols)),
                                 shape=interactions.shape)
    # generating numerator...
    interactions.data = np.log10(interactions.data)
    numerator = interactions - doc_sums
    # generating denominator...
    denominator = interactions
    return numerator, denominator


def main(B):
    # Compute data necessary to compute NPMI every epoch
    reference_vocabulary = "reference/ref.vocab.json"
    ref_counts = "reference/ref.npz"
    ref_vocab = read_json(reference_vocabulary)
    ref_vocab_index = dict(zip(ref_vocab, range(len(ref_vocab))))
    # Loading reference count matrix.
    ref_count_mat = load_sparse(ref_counts)
    # Computing word interaction matrix.
    ref_doc_counts = (ref_count_mat > 0).astype(float)
    ref_interaction = (ref_doc_counts).T.dot(ref_doc_counts)
    ref_doc_sum = np.array(ref_doc_counts.sum(0).tolist()[0])
    # Generating npmi matrices (which will be directly used to calculate nmpi).
    (npmi_numerator,
     npmi_denominator) = generate_npmi_vals(ref_interaction, ref_doc_sum)

    n_docs = ref_count_mat.shape[0]
    vocabulary = json.load(open("vocabulary.json", "r"))
    
    # This function will extract top K words from each topic, you want to make sure K >= 10
    topics = extract_topics(B, list(vocabulary.keys()))
    # This function will compute the npmi, By default, we use only 10 words to calculate npmi
    return compute_npmi(npmi_numerator, npmi_denominator, n_docs, ref_vocab_index, topics)

"""Load Training Documents to Sparse Pytorch Tensor"""

def toSparseTensor(Coo_Matrix): 
    '''
    Converts sparse coo sparse matrix to sparse pytorch tensor
    '''
    coo = Coo_Matrix.tocoo()

    # Convert sparse coo matrix to sparse tensor
    values = coo.data
    indices = np.vstack((coo.row, coo.col))

    i = torch.LongTensor(indices)
    v = torch.FloatTensor(values)
    shape = coo.shape

    train = torch.sparse.FloatTensor(i, v, torch.Size(shape))

    return train

"""Topic Model Neural Network

We present the class that will model the document bag of words
"""

class TopicMod(nn.Module):
    ''' 

    '''

    def __init__(self, NVocab, h, NTopics):
        super(TopicMod, self).__init__()

        self.NTopics = NTopics
      
        self.fc_h = nn.Sequential(nn.Linear(NVocab,h), 
                                 nn.ReLU(), 
                                 nn.Linear(h,h), 
                                 nn.ReLU())
        
        self.mu = nn.Linear(h,NTopics)
        self.sigma = nn.Linear(h,NTopics)
        self.B = nn.Linear(NTopics, NVocab, bias = False)

    def forward(self, x):
        h = self.fc_h(x) # h layer 
        mu_x = self.mu(h) # mu layer 
        sig_x = torch.exp(self.sigma(h)) # sigma layer with exponential trams
        z = mu_x + torch.mul(sig_x, torch.normal(0, 1,
                                                 size=(1, self.NTopics)).cuda()) 
        
        softM = nn.Softmax(dim=1)

        theta = softM(z)
        eta = softM(self.B(theta))
        
        logp = torch.sum(x.to_dense()*torch.log10(eta),1) 
        
        self.logp = logp
        
        self.mu_x = mu_x
        self.sig_x = sig_x
        self.theta = theta
        
        return logp
    
    def loss(self, x, alpha): 
        
        logp = self.forward(x)
        
        KL = self.mu_x**2 + self.sig_x**2 - torch.ones(self.NTopics).cuda() 
        - torch.log(self.sig_x**2) 
        
        KL = alpha*torch.sum(KL,1)/2
        
        loss = -1*torch.mean(logp - KL)
        
        return loss

def alphapara(alphaold, LinScale): 
    '''
    Updates the scalar to update the variational inference term KL 
    to an increasting weight

    Parameters
    ----------
    alphaold : 'float'
        Previous iteration of alpha
    LinScale : 'int'
        Linear parameter for which it increase the alpha term 

    Returns
    -------
    alphanew : 'float'
        New alpha

    '''
    
    alphanew = alphaold + 1/LinScale
    
    return min(alphanew,1)


def train(dataloader, model, num_epochs =100, batchSize = 100, 
          learning_rate = 0.001, linScale = 1000, P = 5): 
    '''
    

    Parameters
    ----------
    dataloader : 'torch.tensor'
        data loader to training 
    model : 'nn.module'
        Instantiated nn.module model for training
    num_epochs : 'int'
        Max number of epochs to evaluate
    learning_rate : 'float'
        learning rate for optimizer 
    linScale: 'Int' 
        Linear annealing for loss function 
    P: 'int'
        Early stopping criteria, if npmi score does not increase for 
        P epochs, training terminates 

    Returns
    -------
    Losses: list 
        returns loss per epoch

    Npmis: list 
        npmi score per epoch

    '''
    # Use SGD optimizer 
    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

    losses, npmis = [], []
    
    # Initiate alpha and Pcount
    alpha_old= 0
    PCount = 0
    
    for epoch in range(num_epochs):

        # Create batches of data 
        idx = np.random.permutation(dataloader.shape[0]) 
        sizeB = int(dataloader.shape[0]/batchSize)
        batches = np.split(idx, sizeB)

        # Increase linScale 
        alpha_new = alphapara(alpha_old, linScale)

        for batch in batches:
            data = dataloader[batch]

            # Convert to sparse tensor
            data = toSparseTensor(data)

            # If you are using a GPU, speed up computation by moving values to the GPU
            if torch.cuda.is_available():
                model = model.cuda()
                data = data.cuda()
                
            optimizer.zero_grad()               # Reset gradient for next computation
            loss = model.loss(data, alpha_new)  # Compute loss
            loss.backward()                     # Backward pass: compute the weight
            optimizer.step()                    # Optimizer: update the weights of hidden nodes

        # update alpha after each epoch     
        alpha_old = alpha_new 

        # Extract updated B matrix from model after each epoch
        B = torch.transpose(model.B.weight,0,1)

        # use helper function provided to calculate npmi after each epoch
        npmi = main(B)

        print(f'Epoch: {epoch} loss is: {loss.item()} and npmi is {npmi}')

        npmis.append(npmi)
        losses.append(loss.item())

        # Check if NPMI continues to increase 
        if epoch == 0: 
            PCount += 1
            npmiold = npmi
        elif npmi <= npmiold: 
            PCount += 1
        else: 
            PCount = 0

        npmiold = npmi

        print(PCount)

        if PCount >= P: 
            break

    return losses, npmis

train_loader = load_sparse('train.npz')

model = TopicMod(30001,81,20)

losses, npmis = train(train_loader, model)

epochs = list(range(1,len(losses)+1))

fig, (ax1, ax2) = plt.subplots(2)
fig.suptitle('Losses and NPMI')
ax1.plot(epochs, losses)
ax1.set_ylabel('loss')
ax2.plot(epochs, npmis)
ax2.set_ylabel('NPMI')
ax2.set_xlabel('Epochs')

"""Evaluate topics"""

vocab = json.load(open("vocabulary.json", "r"))
words = list(vocab.keys())

# Retrieve the top 10 words for the t topic

# Top words list 
TopwordsL = [] 

# retrieve model B layer 
B = model.B.weight.cpu()

for topic in range(5): 
    theta = torch.zeros(20)
    theta[topic] = 1

    softM = nn.Softmax(dim=0)

    eta = softM(B.matmul(theta))

    topidx = torch.argsort(eta, descending = True)[:10]

    topWords = [words[idx] for idx in topidx]

    TopwordsL.append(topWords)

    print('Top words for topic {}:\n' .format(topic))

    for word in topWords: 
        print(word + '\n')