# -*- coding: utf-8 -*-
"""TransferLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eU3Qn59iKlTF8TP-Cszs1VNJVMi5l6K7

I will the google colab environment to utilize their GPUs to speed up the operations.
"""

import math 
import matplotlib.pyplot as plt              # For plotting
import seaborn as sns                        # For styling plots
import torch                                 # Overall PyTorch import
import torch.nn as nn                        # For specifying neural networks
import torch.nn.functional as F              # For common functions
import torch.optim as optim                  # For optimizizing model parameter
from torchvision import datasets, models, transforms
import numpy as np

"""Define some constants"""

INPUT_SIZE    = 3 * 32 * 32   # An image has 32 x 32 pixels each with Red/Green/Blue values. 
NUM_CLASSES   = 10            # The number of output classes. In this case, from 0 to 9
NUM_EPOCHS    = 12            # The number of times we loop over the whole dataset during training
BATCH_SIZE    = 100           # The size of input data took for one iteration of an epoch
LEARNING_RATE = 0.0005         # The speed of convergence

"""Upload CIFAR 10 Dataset"""

transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Resize((256,256)),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = datasets.CIFAR10(root='./data', train=True, download=True, 
                            transform=transform)
# For Smaller batches
trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])

train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, 
                                           shuffle=True, num_workers=2)

val_loader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, 
                                           shuffle=False, num_workers=2)

testset = datasets.CIFAR10(root='./data', train=False, download=True, 
                           transform=transform)

#testset, _ = torch.utils.data.random_split(testset, [BATCH_SIZE * 10, len(testset) - BATCH_SIZE * 10])
test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, 
                                          shuffle=False, num_workers=2)

"""Define your critical helper fuctions for training and augmenting AlexNet model to fit the CIFAR 10 dataset"""

def train(model, train_loader, val_loader,
          num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
          compute_accs = False): 
    """
    This function uses the model to train on the training data for the 
    specified number of epochs. If compute_accs is set to True, the training 
    and validation accuracies 

    Args:
        model: Modified Alexnet Model
        train_loader, val_loader: The pytorch dataset loaders for the trainst and valset
        num_epochs: The number of times to loop over the batches in train_loader
        learning_rate: The learning rate for the optimizer
        compute_accs: A bool flag for whether or not this function should compute the train and validation
                      accuracies at the end of each epoch. This feature is useful for visualizing
                      how the model is learning, but slows down training time.
    Returns:
        The train and validation accuracies if compute_accs is True, None otherwise
    """

    # Create loss Function

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE) 

    train_accs = [] 
    val_accs = [] 
    train_loss = []
    val_loss = []

    for epoch in range(num_epochs): 
        for i, (img, labels) in enumerate(train_loader): 

            # If you are using a GPU, speed up computation by moving values to the GPU
            if torch.cuda.is_available():
                model = model.cuda()
                img = img.cuda()
                labels = labels.cuda()

            optimizer.zero_grad()               # Reset gradient for next computation
            outputs = model(img)               # Forward pass: compute the output class given a image
            loss = criterion(outputs, labels)   # Compute loss: difference between the pred and true
            loss.backward()                     # Backward pass: compute the weight
            optimizer.step()                    # Optimizer: update the weights of hidden node
                

        train_acc = accuracy(model,train_loader)
        val_acc = accuracy(model,val_loader)
        trainL = loss.item()
        valL =  lossData(model, val_loader)

        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Acc {100 * train_acc:.2f}%, Validation Acc {100 * val_acc:.2f}%')
        
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        train_loss.append(trainL)
        val_loss.append(valL)


    if compute_accs:
        return train_accs, val_accs, train_loss, val_loss
    else:
        return None

def accuracy(model, data_loader):
    """
    For a given data_loader, evaluate the model on the dataset and compute its classification
    accuracy.

    Args:
        model: The neural network to train (Alexnet)
        data_loader: A dataset loader for some dataset.
    Returns:
        The classificiation accuracy of the model on this dataset.
    """
    correct = 0
    total = 0
    for images, labels in data_loader:
        if  torch.cuda.is_available():
            images = images.cuda()
            labels = labels.cuda()

        outputs = model(images)                           # Make predictions
        _, predicted = torch.max(outputs.data, 1)       # Choose class with highest scores
        total += labels.size(0)                         # Increment the total count
        correct += (predicted == labels).sum().item()   # Increment the correct count

    return correct / total

def lossData(model, dataloader):
    '''
    Provides current loss based on latest model update

    Args: 
        model: current working model
        lossfn: Defined loss function
        dataloader: Data to perform loss on
    '''
    losses = []

    for images, labels in dataloader:
        images = images.cuda()
        labels = labels.cuda()

        model = model.cuda()

        criterion = nn.CrossEntropyLoss()

        outputs = model(images)    # Forward pass: compute the output class given a image
        loss = criterion(outputs, labels)

        losses.append(loss.item())


    avg_loss = np.mean(np.array(losses))

    return avg_loss

def plot(train_met, val_met, Accuracy = True):
    """
    Plots and training and validation accuracies over times

    Args: 
        train_acc, val_acc: list of training/validation accuracies over training time

    """

    plt.figure(figsize = (16,10))
    epochs = list(range(1, len(train_met) + 1))

    val = plt.plot(epochs, val_met,
                     '--', label= 'Validation')
    
    plt.plot(epochs, train_met, color=val[0].get_color(),
               label= 'Train')
    
    if Accuracy: 
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title('Accuracy vs Epochs')
        plt.legend()
        plt.xlim([1,max(epochs)])

    else: 
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Loss vs Epochs')
        plt.legend()
        plt.xlim([1,max(epochs)])

"""Main function"""

# modify Alexnet model 
model = models.alexnet(pretrained=True)
for param in model.parameters():
  param.requires_grad = False
model.classifier[6] = nn.Linear(4096, NUM_CLASSES)

# Train the model and print progress
train_accs, val_accs, train_loss, val_loss = train(model, train_loader, val_loader,
                             num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
                             compute_accs = True)

plot(train_accs, val_accs)
plot(train_loss, val_loss, Accuracy = False)

print('Test accuracy of model is : %.3f' % accuracy(model, test_loader))
print('Test loss of model is : %.4f' % lossData(model, test_loader))

"""**Part B**

Now instead of just training the last layer, we will use Alexnet to train the entire network, as oppose to freezing the layers. 
"""

# modify Alexnet model 
model2 = models.alexnet(pretrained=True)
model2.classifier[6] = nn.Linear(4096, NUM_CLASSES)

# Train the model and print progress
train_accs2, val_accs2, train_loss2, val_loss2  = train(model2, train_loader, val_loader,
                             num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
                             compute_accs = True)

plot(train_accs2, val_accs2)
plot(train_loss2, val_loss2, Accuracy = False)

print('Test accuracy of model is : %.3f' % accuracy(model2, test_loader))
print('Test loss of model is : %.4f' % lossData(model2, test_loader))